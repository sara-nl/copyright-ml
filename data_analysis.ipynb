{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and take a look\n",
    "\n",
    "Let's start by looking at what columns we have, what their data types are and how many null-values there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1601 entries, 0 to 1600\n",
      "Data columns (total 80 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   FacultyName                    1600 non-null   object \n",
      " 1   CourseName                     1600 non-null   object \n",
      " 2   PredictionSurf                 1195 non-null   object \n",
      " 3   PredictionInstitution          876 non-null    object \n",
      " 4   PredictionRemark               360 non-null    object \n",
      " 5   CorrectDoi                     69 non-null     object \n",
      " 6   CorrectISBN                    71 non-null     object \n",
      " 7   AnalyseError                   385 non-null    object \n",
      " 8   CorrectAnalyseSurf             1601 non-null   int64  \n",
      " 9   CorrectAnalyseInstitution      1601 non-null   int64  \n",
      " 10  AnalyseISBN                    1600 non-null   float64\n",
      " 11  AnalyseDOI                     1600 non-null   float64\n",
      " 12  id                             1600 non-null   float64\n",
      " 13  uuid                           1600 non-null   object \n",
      " 14  url                            1600 non-null   object \n",
      " 15  filesource                     1600 non-null   object \n",
      " 16  filestatus                     1600 non-null   float64\n",
      " 17  filemimetype                   1600 non-null   object \n",
      " 18  filename                       1600 non-null   object \n",
      " 19  filehash                       1289 non-null   object \n",
      " 20  filedate                       1599 non-null   object \n",
      " 21  lastmodifieddate               749 non-null    object \n",
      " 22  creator                        713 non-null    object \n",
      " 23  isfilepublished                1597 non-null   object \n",
      " 24  wordcount                      1597 non-null   float64\n",
      " 25  pagecount                      1597 non-null   float64\n",
      " 26  filescanresults                1597 non-null   float64\n",
      " 27  doi                            192 non-null    object \n",
      " 28  issn                           0 non-null      float64\n",
      " 29  isbn                           92 non-null     object \n",
      " 30  author                         353 non-null    object \n",
      " 31  title                          348 non-null    object \n",
      " 32  publisher                      71 non-null     object \n",
      " 33  publicationyear                163 non-null    float64\n",
      " 34  sourcepagecount                1589 non-null   float64\n",
      " 35  sourcewordcount                1589 non-null   float64\n",
      " 36  usedpages                      0 non-null      float64\n",
      " 37  filetype                       1589 non-null   float64\n",
      " 38  oclcnumber                     1589 non-null   float64\n",
      " 39  incollection                   1589 non-null   object \n",
      " 40  userexcludedforscan            1589 non-null   object \n",
      " 41  usedmultiplesources            1589 non-null   object \n",
      " 42  isopenaccesstitle              1589 non-null   object \n",
      " 43  openaccesslink                 62 non-null     object \n",
      " 44  filepath                       0 non-null      float64\n",
      " 45  runidentifier                  0 non-null      float64\n",
      " 46  picturecount                   1589 non-null   float64\n",
      " 47  prediction                     1280 non-null   object \n",
      " 48  reliability                    1589 non-null   float64\n",
      " 49  jstor                          1280 non-null   object \n",
      " 50  always                         1280 non-null   object \n",
      " 51  DOI_in_OA                      1280 non-null   object \n",
      " 52  DOI_no_PPT                     1280 non-null   object \n",
      " 53  PPT_in_name                    1280 non-null   object \n",
      " 54  ppt_creator                    1280 non-null   object \n",
      " 55  wordcount_o                    1280 non-null   object \n",
      " 56  _10_pics_page                  0 non-null      float64\n",
      " 57  Contains_DOI                   1280 non-null   object \n",
      " 58  Contains_ISBN                  1280 non-null   object \n",
      " 59  creator_abbyy                  1280 non-null   object \n",
      " 60  WordsPage350                   1280 non-null   object \n",
      " 61  keyword_creator                1280 non-null   object \n",
      " 62  Words_more_300pp               1280 non-null   object \n",
      " 63  file_ext_mp3_wav               1280 non-null   object \n",
      " 64  file_ext_mp4_mov               1280 non-null   object \n",
      " 65  _10Pagecount50                 1280 non-null   object \n",
      " 66  Contains_copyright             1280 non-null   object \n",
      " 67  Kleiner_10_paginas             1280 non-null   object \n",
      " 68  filename_indicator             1280 non-null   object \n",
      " 69  Contains_sciencemag            1280 non-null   object \n",
      " 70  Pagecount_bigger_50            1280 non-null   object \n",
      " 71  BookAndWords10000              1280 non-null   object \n",
      " 72  Contains_published_in          1280 non-null   object \n",
      " 73  Contains_researchgate          1280 non-null   object \n",
      " 74  Contains_to_appear_in          1280 non-null   object \n",
      " 75  IsJournalWords8000             1280 non-null   object \n",
      " 76  images_same_pagecount          1280 non-null   object \n",
      " 77  Publisher_from_crossref        1280 non-null   object \n",
      " 78  Contains_recommended_citation  1280 non-null   object \n",
      " 79  Kolom1                         0 non-null      float64\n",
      "dtypes: float64(20), int64(2), object(58)\n",
      "memory usage: 1000.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    './outputSURF-AI-testset.csv',\n",
    "    sep=';'\n",
    ")\n",
    "\n",
    "df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionSurfNormalized</th>\n",
       "      <th>PredictionToolNormalized</th>\n",
       "      <th>FacultyName</th>\n",
       "      <th>CourseName</th>\n",
       "      <th>PredictionSurf</th>\n",
       "      <th>PredictionInstitution</th>\n",
       "      <th>PredictionRemark</th>\n",
       "      <th>CorrectDoi</th>\n",
       "      <th>CorrectISBN</th>\n",
       "      <th>Level2</th>\n",
       "      <th>...</th>\n",
       "      <th>Contains_sciencemag</th>\n",
       "      <th>Pagecount_bigger_50</th>\n",
       "      <th>BookAndWords10000</th>\n",
       "      <th>Contains_published_in</th>\n",
       "      <th>Contains_researchgate</th>\n",
       "      <th>Contains_to_appear_in</th>\n",
       "      <th>IsJournalWords8000</th>\n",
       "      <th>images_same_pagecount</th>\n",
       "      <th>Publisher_from_crossref</th>\n",
       "      <th>Contains_recommended_citation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Metabolic Consequences of Chronic Diseases wit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Human Nutrition and Health</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Eigen Materiaal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Disease Ecology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wildlife Ecology and Conservation Group</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agrobiodiversity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Soil Biology</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Management Skills in Theory &amp; Practice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Education and Learning Sciences</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thesis Skills</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rural Sociology</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PredictionSurfNormalized PredictionToolNormalized  FacultyName  \\\n",
       "0                      NaN                      NaN          NaN   \n",
       "1                      NaN          Eigen Materiaal          NaN   \n",
       "2                      NaN                      NaN          NaN   \n",
       "3                      NaN                      NaN          NaN   \n",
       "4                      NaN                      NaN          NaN   \n",
       "\n",
       "                                          CourseName PredictionSurf  \\\n",
       "0  Metabolic Consequences of Chronic Diseases wit...            NaN   \n",
       "1                                    Disease Ecology            NaN   \n",
       "2                                   Agrobiodiversity            NaN   \n",
       "3             Management Skills in Theory & Practice            NaN   \n",
       "4                                      Thesis Skills            NaN   \n",
       "\n",
       "  PredictionInstitution PredictionRemark CorrectDoi CorrectISBN  \\\n",
       "0                   NaN              NaN        NaN         NaN   \n",
       "1                   NaN              NaN        NaN         NaN   \n",
       "2                   NaN              NaN        NaN         NaN   \n",
       "3                   NaN              NaN        NaN         NaN   \n",
       "4                   NaN              NaN        NaN         NaN   \n",
       "\n",
       "                                    Level2  ... Contains_sciencemag  \\\n",
       "0               Human Nutrition and Health  ...                 NaN   \n",
       "1  Wildlife Ecology and Conservation Group  ...               False   \n",
       "2                             Soil Biology  ...                 NaN   \n",
       "3          Education and Learning Sciences  ...                 NaN   \n",
       "4                          Rural Sociology  ...                 NaN   \n",
       "\n",
       "   Pagecount_bigger_50  BookAndWords10000  Contains_published_in  \\\n",
       "0                  NaN                NaN                    NaN   \n",
       "1                False              False                  False   \n",
       "2                  NaN                NaN                    NaN   \n",
       "3                  NaN                NaN                    NaN   \n",
       "4                  NaN                NaN                    NaN   \n",
       "\n",
       "  Contains_researchgate  Contains_to_appear_in  IsJournalWords8000  \\\n",
       "0                   NaN                    NaN                 NaN   \n",
       "1                 False                  False               False   \n",
       "2                   NaN                    NaN                 NaN   \n",
       "3                   NaN                    NaN                 NaN   \n",
       "4                   NaN                    NaN                 NaN   \n",
       "\n",
       "   images_same_pagecount  Publisher_from_crossref  \\\n",
       "0                    NaN                      NaN   \n",
       "1                  False                    False   \n",
       "2                    NaN                      NaN   \n",
       "3                    NaN                      NaN   \n",
       "4                    NaN                      NaN   \n",
       "\n",
       "   Contains_recommended_citation  \n",
       "0                            NaN  \n",
       "1                          False  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following columns contain only null values and will not be used:\n",
    "\n",
    "<ul>\n",
    "    <li> 'issn'\n",
    "    <li> 'usedpages'\n",
    "    <li> 'filepath'\n",
    "    <li> 'runidentifier'\n",
    "    <li> '_10_pics_page'\n",
    "    <li> 'Kolom1'\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].isna().all():\n",
    "        print(col)\n",
    "df.dropna(axis=1, how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following columns concern the predictions and will not be used as input:\n",
    "\n",
    "<ul>\n",
    "    <li> 'PredictionSurf'\n",
    "    <li> 'PredictionInstitution' (ground truth)\n",
    "    <li> 'PredictionRemark'\n",
    "    <li> 'prediction'\n",
    "</ul>\n",
    "\n",
    "The column 'PredictionInstitution' will be used as label, therefore all rows where this column has a null-value will be dropped. The others will be dropped completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PredictionSurf', 'prediction', 'PredictionRemark'], axis=\"columns\", inplace=True)\n",
    "df.dropna(subset=['PredictionInstitution'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following columns seem to contain identifiers and the like:\n",
    "\n",
    "<ul>\n",
    "    <li> 'FacultyName'\n",
    "    <li> 'CourseName'\n",
    "    <li> 'CourseName'\n",
    "    <li> 'CorrectDoi'\n",
    "    <li> 'CorrectISBN'\n",
    "    <li> 'AnalyseError'\n",
    "    <li> 'CorrectAnalyseSurf'\n",
    "    <li> 'CorrectAnalyseInstitution'\n",
    "    <li> 'AnalyseISBN'\n",
    "    <li> 'AnalyseDOI'\n",
    "    <li> 'id'\n",
    "    <li> 'uuid'\n",
    "    <li> 'url'\n",
    "    <li> 'filesource'\n",
    "    <li> 'filestatus'\n",
    "    <li> 'filemimetype'\n",
    "    <li> 'filename'\n",
    "    <li> 'filehash'\n",
    "    <li> 'filedate'\n",
    "    <li> 'lastmodifieddate'\n",
    "    <li> 'creator'\n",
    "    <li> 'isfilepublished'\n",
    "    <li> 'filescanresults'\n",
    "    <li> 'doi'\n",
    "    <li> 'isbn'\n",
    "    <li> 'author'\n",
    "    <li> 'title'\n",
    "    <li> 'publicationyear'\n",
    "    <li> 'oclcnumber'\n",
    "</ul>\n",
    "\n",
    "These columns will not be used.\n",
    "\n",
    "#### I PLAN TO TAKE A CLOSER LOOK AT AT LEAST SOME OF THESE, like title which might become a useful feature with the application of NLP, or 'CorrectAnalyse' columns, which may tell us something about the tool's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_cols = ['FacultyName', 'CourseName', 'CorrectDoi', 'CorrectISBN',\n",
    "'AnalyseError', 'CorrectAnalyseSurf', 'CorrectAnalyseInstitution',\n",
    "'AnalyseISBN', 'AnalyseDOI', 'id', 'uuid', 'url', 'filesource',\n",
    "'filestatus', 'filemimetype', 'filename', 'filehash', 'filedate',\n",
    "'lastmodifieddate', 'creator', 'isfilepublished', 'filescanresults',\n",
    "'doi', 'isbn', 'author', 'title', 'publicationyear', 'oclcnumber']\n",
    "\n",
    "df.drop(identifier_cols, axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take another look at our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a separate look at columns with dtype object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "len(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most columns with dtype object contain boolean values. We will change these to 0s and 1s and change their dtype to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in object_cols:\n",
    "    if set(df[col].dropna().unique()) == {False, True}:\n",
    "        df.loc[df[col] == True, col] = 1\n",
    "        df.loc[df[col] == False, col] = 0\n",
    "        df[col] = df[col].fillna(0)\n",
    "        df[col] = df[col].astype(\"int\")\n",
    "    elif len(set(df[col].dropna().unique())) == 1:\n",
    "        df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a profiling report\n",
    "\n",
    "Uncomment and run this cell to get a pandas profiling report. This will show nicely which features are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install -U pandas-profiling[notebook]\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "# profile = ProfileReport(df.reset_index(drop=True), title=\"Pandas Profiling Report\")\n",
    "# profile.to_file(\"pandas_report1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant values\n",
    "\n",
    "There are 2 columns with a constant value: 'sourcepagecount' and 'sourcewordcount'. These columns will be dropped.\n",
    "\n",
    "#### QUESTION: What are these columns? How does it relate to 'pagecount' and 'wordcount'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"sourcepagecount\", \"sourcewordcount\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last columns of dtype object\n",
    "\n",
    "The last three columns of dtype object consist of 2 non-boolean features and 1 column with the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "len(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in object_cols:\n",
    "    print(f\"column name: {col}\")\n",
    "    print(f\"number of unique values: {len(df[col].unique())}\")\n",
    "    print(f\"unique values: {df[col].unique()}\")\n",
    "    print(f\"number of null values: {df[col].isna().sum()}\")\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop openaccesslink\n",
    "\n",
    "The 'openaccesslink' feature has 15 unique values, which are uniformly distributed and only once or twice each. This feature has 98% missing values; whether this value is missing highly correlates with the value of 'isopenaccesstitle'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = df.loc[df[\"isopenaccesstitle\"] == 1]\n",
    "print(f\"Number of datapoints that are open access titles:\\t\\t{len(oa)}.\\nNumber of open access titles that have an open access link:\\t{oa['openaccesslink'].notna().sum()}.\")\n",
    "\n",
    "print(\"==========================================================================\")\n",
    "\n",
    "not_oa = df.loc[df[\"isopenaccesstitle\"] == 0]\n",
    "print(f\"Number of datapoints that are NOT open access titles:\\t\\t{len(not_oa)}.\\nNumber of non open access titles that have an open access link:\\t{not_oa['openaccesslink'].notna().sum()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"openaccesslink\", axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop publisher\n",
    "\n",
    "The 'publisher' feature is highly correlated with many features, such as wordcount, contains_researchgate, Words_more_300pp and several others. It also has 93% missing values and 33 unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"publisher\", axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels' support\n",
    "\n",
    "We lost many entries, mostly because they had no value for 'PredictionInstitution', which is our ground truth. Let's take a look at the support of each label, i.e., how often does each label occur in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"PredictionInstitution\"]\n",
    "unique_labels = labels.unique()\n",
    "\n",
    "for label in unique_labels:\n",
    "    print('{:<32}  {:>3}'.format(label, (labels == label).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: Difference overname open access and open access? zelfde!!   eigen materiaal ook op een hoop (behalve powerpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have enough examples of each class. Let's see what happens if we drop columns with 1 or 2 samples and train only on labels we have more examples of. We also change the label 'overname middellang' to 'middellange overname', since this was probably a labeling error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_subset = df[df.PredictionInstitution != \"overname open access\"]\n",
    "df_label_subset = df_label_subset[df_label_subset.PredictionInstitution != \"mogelijk licentie\"]\n",
    "df_label_subset = df_label_subset[df_label_subset.PredictionInstitution != \"overname met licentie\"]\n",
    "\n",
    "df_label_subset.loc[df_label_subset[\"PredictionInstitution\"] == \"overname middellang\", \"PredictionInstitution\"] = \"middellange overname\"\n",
    "\n",
    "labels = df_label_subset[\"PredictionInstitution\"]\n",
    "unique_labels = labels.unique()\n",
    "\n",
    "for label in unique_labels:\n",
    "    print('{:<32}  {:>3}'.format(label, (labels == label).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_label_subset.drop(\"PredictionInstitution\", axis=\"columns\").to_numpy()\n",
    "\n",
    "print(x.shape)\n",
    "print(x[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y = label_encoder.fit_transform(df_label_subset[\"PredictionInstitution\"])\n",
    "\n",
    "print(y.shape)\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model\n",
    "\n",
    "### High correlation\n",
    "\n",
    "There are many features with high correlation. Since tree-based models are not so sensitive to this, let's train XGBoost on our data. This model achieves both accuracy and F1 score of 0.98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "predictions = xgb_model.predict(x_test)\n",
    "report = classification_report(y_test, predictions, target_names=label_encoder.classes_, labels=np.unique(y_train))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels' support\n",
    "\n",
    "We don't have enough examples of each class. In an attempt to combat this, we will add some datapoints to our training data that do not have a value for 'PredictionInstitution', but DO have a value for 'prediction'. We also process it the same way we processed the rest of the data above, by removing the same columns etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\n",
    "    './outputSURF-AI-testset.csv',\n",
    "    sep=';'\n",
    ")\n",
    "\n",
    "df2.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "df2.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "df2 = df2[df2['PredictionInstitution'].isna()]\n",
    "\n",
    "df2.drop(['PredictionSurf', 'PredictionInstitution', 'PredictionRemark'], axis=\"columns\", inplace=True)\n",
    "df2.dropna(subset=['prediction'], inplace=True)\n",
    "\n",
    "drop_cols = ['FacultyName', 'CourseName', 'CorrectDoi', 'CorrectISBN',\n",
    "'AnalyseError', 'CorrectAnalyseSurf', 'CorrectAnalyseInstitution',\n",
    "'AnalyseISBN', 'AnalyseDOI', 'id', 'uuid', 'url', 'filesource',\n",
    "'filestatus', 'filemimetype', 'filename', 'filehash', 'filedate',\n",
    "'lastmodifieddate', 'creator', 'isfilepublished', 'filescanresults',\n",
    "'doi', 'isbn', 'author', 'title', 'publicationyear',\n",
    "'filetype', 'oclcnumber', 'sourcepagecount', 'sourcewordcount', 'publisher', 'openaccesslink', 'Contains_sciencemag', 'creator_abbyy']\n",
    "\n",
    "df2.drop(drop_cols, axis=\"columns\", inplace=True)\n",
    "\n",
    "object_cols = [col for col in df2.columns if df2[col].dtype == 'object']\n",
    "\n",
    "for col in object_cols:\n",
    "    if set(df2[col].dropna().unique()) == {False, True}:\n",
    "        df2.loc[df2[col] == True, col] = 1\n",
    "        df2.loc[df2[col] == False, col] = 0\n",
    "        df2[col] = df2[col].fillna(0)\n",
    "        df2[col] = df2[col].astype(\"int\")\n",
    "    elif len(set(df2[col].dropna().unique())) == 1:\n",
    "        df2.drop(col, axis=1, inplace=True)\n",
    "\n",
    "df2.dropna(inplace=True)\n",
    "df2.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df2.drop(\"prediction\", axis=\"columns\").to_numpy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(df2[\"prediction\"])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x\n",
    "y_test = y\n",
    "\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(x_train, y_train)\n",
    "\n",
    "predictions = xgb_model.predict(x_test)\n",
    "report = classification_report(y_test, predictions, target_names=label_encoder.classes_, labels=np.unique(y_train))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add to train set\n",
    "\n",
    "Instead of treating the data with 'prediction' as its label as the whole training set, let's split the data with 'PredictionInstitution' and add 'prediction' data to the train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\n",
    "    './outputSURF-AI-testset.csv',\n",
    "    sep=';'\n",
    ")\n",
    "\n",
    "df3.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "df3.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "df3.drop(['PredictionSurf', 'prediction', 'PredictionRemark'], axis=\"columns\", inplace=True)\n",
    "df3.dropna(subset=['PredictionInstitution'], inplace=True)\n",
    "\n",
    "drop_cols = ['FacultyName', 'CourseName', 'CorrectDoi', 'CorrectISBN',\n",
    "'AnalyseError', 'CorrectAnalyseSurf', 'CorrectAnalyseInstitution',\n",
    "'AnalyseISBN', 'AnalyseDOI', 'id', 'uuid', 'url', 'filesource',\n",
    "'filestatus', 'filemimetype', 'filename', 'filehash', 'filedate',\n",
    "'lastmodifieddate', 'creator', 'isfilepublished', 'filescanresults',\n",
    "'doi', 'isbn', 'author', 'title', 'publicationyear',\n",
    "'filetype', 'oclcnumber', 'sourcepagecount', 'sourcewordcount', 'publisher', 'openaccesslink', 'Contains_sciencemag', 'creator_abbyy']\n",
    "\n",
    "df3.drop(drop_cols, axis=\"columns\", inplace=True)\n",
    "\n",
    "object_cols = [col for col in df3.columns if df3[col].dtype == 'object']\n",
    "\n",
    "for col in object_cols:\n",
    "    if set(df3[col].dropna().unique()) == {False, True}:\n",
    "        df3.loc[df3[col] == True, col] = 1\n",
    "        df3.loc[df3[col] == False, col] = 0\n",
    "        df3[col] = df3[col].fillna(0)\n",
    "        df3[col] = df3[col].astype(\"int\")\n",
    "    elif len(set(df3[col].dropna().unique())) == 1:\n",
    "        df3.drop(col, axis=1, inplace=True)\n",
    "\n",
    "df3.dropna(inplace=True)\n",
    "df3.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df3[\"PredictionInstitution\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df3.drop(\"PredictionInstitution\", axis=\"columns\").to_numpy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df3[\"PredictionInstitution\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = df2.drop(\"prediction\", axis=\"columns\").to_numpy()\n",
    "y2 = label_encoder.transform(df2[\"prediction\"])\n",
    "\n",
    "x_train_augmented = np.vstack((x_train, x2))\n",
    "y_train_augmented = np.concatenate((y_train, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still underrepresented labels, though it's less than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.unique(y_test):\n",
    "    print(i, (y_train_augmented == i).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.inverse_transform([8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(x_train_augmented, y_train_augmented, labels=np.unique(y_train))\n",
    "\n",
    "predictions = xgb_model.predict(x_test)\n",
    "report = classification_report(y_test, predictions, target_names=label_encoder.classes_, labels=np.unique(y_train))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With K-fold\n",
    "\n",
    "Use K-fold to confirm that the accuracy and F1 is actually that high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n",
    "\n",
    "accuracy_scores = 0.0\n",
    "f1_scores = 0.0\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    xgb_model.fit(x_train, y_train)\n",
    "    predictions = xgb_model.predict(x_test)\n",
    "    accuracy_scores += accuracy_score(y_test, predictions)\n",
    "    f1_scores += f1_score(y_test, predictions, average=\"macro\")\n",
    "\n",
    "print(accuracy_scores/6)\n",
    "print(f1_scores/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative classifiers\n",
    "\n",
    "### Train another model, an SVC, to compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_x = scaler.fit_transform(x)\n",
    "\n",
    "accuracy_scores = 0.0\n",
    "f1_scores = 0.0\n",
    "\n",
    "for train_index, test_index in skf.split(scaled_x, y):\n",
    "    x_train, x_test = scaled_x[train_index], scaled_x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    svc_model = SVC()\n",
    "    svc_model.fit(x_train, y_train)\n",
    "    predictions = svc_model.predict(x_test)\n",
    "    accuracy_scores += accuracy_score(y_test, predictions)\n",
    "    f1_scores += f1_score(y_test, predictions, average=\"macro\")\n",
    "\n",
    "print(accuracy_scores/6)\n",
    "print(f1_scores/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = 0.0\n",
    "f1_scores = 0.0\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(x_train, y_train)\n",
    "    predictions = rf_model.predict(x_test)\n",
    "    accuracy_scores += accuracy_score(y_test, predictions)\n",
    "    f1_scores += f1_score(y_test, predictions, average=\"macro\")\n",
    "\n",
    "print(accuracy_scores/6)\n",
    "print(f1_scores/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_x = scaler.fit_transform(x)\n",
    "\n",
    "accuracy_scores = 0.0\n",
    "f1_scores = 0.0\n",
    "\n",
    "for train_index, test_index in skf.split(scaled_x, y):\n",
    "    x_train, x_test = scaled_x[train_index], scaled_x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lr_model = LogisticRegression()\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    predictions = lr_model.predict(x_test)\n",
    "    accuracy_scores += accuracy_score(y_test, predictions)\n",
    "    f1_scores += f1_score(y_test, predictions, average=\"macro\")\n",
    "\n",
    "print(accuracy_scores/6)\n",
    "print(f1_scores/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "XGBoost has another benefit: it is straightforward to retrieve feature importance scores. Let's take a look at those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df.drop(\"prediction\", axis=\"columns\").columns.to_numpy()\n",
    "feature_importance_scores = xgb_model.feature_importances_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_five_indices = np.argpartition(feature_importance_scores, (-5, -1))[-5:]\n",
    "top_five_scores = feature_importance_scores[top_five_indices][::-1]\n",
    "top_five_names = feature_names[top_five_indices][::-1]\n",
    "\n",
    "for name, score in zip(top_five_names, top_five_scores):\n",
    "    print(f\"{name},   {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score is zero\n",
    "\n",
    "Some of the features have importance scores of 0. When we leave them out, we get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_score_indices = np.argwhere(feature_importance_scores == 0).flatten()\n",
    "zero_score_features = feature_names[zero_score_indices]\n",
    "\n",
    "for feature in zero_score_features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = list(zero_score_features)\n",
    "features_to_remove.append(\"prediction\")\n",
    "\n",
    "x = df.drop(features_to_remove, axis=\"columns\").to_numpy()\n",
    "\n",
    "print(x.shape)\n",
    "print(x[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgb_model.predict(x_test)\n",
    "\n",
    "report = classification_report(y_test, predictions, target_names=label_encoder.classes_)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "054e63ccd0723eba59e4e775f028e662383a4d3e051ddb13b43bfc67a74f1731"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
